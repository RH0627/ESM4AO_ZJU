{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f083e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_new = pd.read_csv('data/data_clean.csv', delimiter=',',header = None,index_col= 0, skiprows=1)\n",
    "X_out = pd.read_csv('generalization_data/X_test_nr_320.csv',delimiter=',',header = None,index_col= 0, skiprows=1)\n",
    "X_out_Sequence = pd.read_csv('generalization_data/X_test_nr.csv',delimiter=',',header = None,index_col= 0, dtype={0: int},skiprows=1)\n",
    "# load numpy array from csv file\n",
    "from numpy import loadtxt\n",
    "import numpy as np\n",
    "# load array\n",
    "y_new = loadtxt('data/y_clean.csv', delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ccda8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.096660</td>\n",
       "      <td>-0.426441</td>\n",
       "      <td>0.313109</td>\n",
       "      <td>0.085290</td>\n",
       "      <td>0.138312</td>\n",
       "      <td>-0.099997</td>\n",
       "      <td>-0.111717</td>\n",
       "      <td>-0.024416</td>\n",
       "      <td>-0.190679</td>\n",
       "      <td>-0.114180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176901</td>\n",
       "      <td>-0.028447</td>\n",
       "      <td>0.213162</td>\n",
       "      <td>0.065364</td>\n",
       "      <td>-0.040807</td>\n",
       "      <td>0.148360</td>\n",
       "      <td>0.205011</td>\n",
       "      <td>0.040825</td>\n",
       "      <td>0.314078</td>\n",
       "      <td>0.049626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.112975</td>\n",
       "      <td>-0.229863</td>\n",
       "      <td>0.499656</td>\n",
       "      <td>-0.036543</td>\n",
       "      <td>0.129535</td>\n",
       "      <td>0.154801</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>-0.182959</td>\n",
       "      <td>-0.005274</td>\n",
       "      <td>-0.275747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063540</td>\n",
       "      <td>0.296249</td>\n",
       "      <td>-0.037545</td>\n",
       "      <td>-0.158626</td>\n",
       "      <td>0.218077</td>\n",
       "      <td>-0.110642</td>\n",
       "      <td>-0.046975</td>\n",
       "      <td>-0.013080</td>\n",
       "      <td>0.198111</td>\n",
       "      <td>-0.166024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008282</td>\n",
       "      <td>-0.399306</td>\n",
       "      <td>0.318922</td>\n",
       "      <td>0.191246</td>\n",
       "      <td>0.139108</td>\n",
       "      <td>-0.186064</td>\n",
       "      <td>-0.196404</td>\n",
       "      <td>-0.093113</td>\n",
       "      <td>-0.123331</td>\n",
       "      <td>-0.112704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172398</td>\n",
       "      <td>0.051751</td>\n",
       "      <td>0.179082</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.014872</td>\n",
       "      <td>0.183214</td>\n",
       "      <td>0.256547</td>\n",
       "      <td>0.104285</td>\n",
       "      <td>0.272940</td>\n",
       "      <td>0.063905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.020909</td>\n",
       "      <td>-0.327159</td>\n",
       "      <td>0.341372</td>\n",
       "      <td>0.135156</td>\n",
       "      <td>0.036137</td>\n",
       "      <td>-0.095794</td>\n",
       "      <td>-0.066457</td>\n",
       "      <td>-0.033484</td>\n",
       "      <td>-0.342004</td>\n",
       "      <td>-0.056031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360627</td>\n",
       "      <td>-0.132805</td>\n",
       "      <td>0.262567</td>\n",
       "      <td>0.185340</td>\n",
       "      <td>-0.137693</td>\n",
       "      <td>0.256558</td>\n",
       "      <td>0.103444</td>\n",
       "      <td>0.079044</td>\n",
       "      <td>0.280564</td>\n",
       "      <td>0.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.212728</td>\n",
       "      <td>-0.301118</td>\n",
       "      <td>0.377535</td>\n",
       "      <td>0.064855</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>-0.235522</td>\n",
       "      <td>-0.109972</td>\n",
       "      <td>-0.006727</td>\n",
       "      <td>-0.149966</td>\n",
       "      <td>-0.257322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021114</td>\n",
       "      <td>0.084950</td>\n",
       "      <td>0.129833</td>\n",
       "      <td>-0.066362</td>\n",
       "      <td>0.177310</td>\n",
       "      <td>0.074210</td>\n",
       "      <td>-0.107300</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.178091</td>\n",
       "      <td>-0.059311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 320 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1         2         3         4         5         6         7    \\\n",
       "0                                                                         \n",
       "0  0.096660 -0.426441  0.313109  0.085290  0.138312 -0.099997 -0.111717   \n",
       "1  0.112975 -0.229863  0.499656 -0.036543  0.129535  0.154801  0.000629   \n",
       "2  0.008282 -0.399306  0.318922  0.191246  0.139108 -0.186064 -0.196404   \n",
       "3 -0.020909 -0.327159  0.341372  0.135156  0.036137 -0.095794 -0.066457   \n",
       "4  0.212728 -0.301118  0.377535  0.064855  0.011884 -0.235522 -0.109972   \n",
       "\n",
       "        8         9         10   ...       311       312       313       314  \\\n",
       "0                                ...                                           \n",
       "0 -0.024416 -0.190679 -0.114180  ...  0.176901 -0.028447  0.213162  0.065364   \n",
       "1 -0.182959 -0.005274 -0.275747  ...  0.063540  0.296249 -0.037545 -0.158626   \n",
       "2 -0.093113 -0.123331 -0.112704  ...  0.172398  0.051751  0.179082  0.106559   \n",
       "3 -0.033484 -0.342004 -0.056031  ...  0.360627 -0.132805  0.262567  0.185340   \n",
       "4 -0.006727 -0.149966 -0.257322  ... -0.021114  0.084950  0.129833 -0.066362   \n",
       "\n",
       "        315       316       317       318       319       320  \n",
       "0                                                              \n",
       "0 -0.040807  0.148360  0.205011  0.040825  0.314078  0.049626  \n",
       "1  0.218077 -0.110642 -0.046975 -0.013080  0.198111 -0.166024  \n",
       "2  0.014872  0.183214  0.256547  0.104285  0.272940  0.063905  \n",
       "3 -0.137693  0.256558  0.103444  0.079044  0.280564  0.204500  \n",
       "4  0.177310  0.074210 -0.107300  0.078431  0.178091 -0.059311  \n",
       "\n",
       "[5 rows x 320 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e9acb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064718</td>\n",
       "      <td>-0.288770</td>\n",
       "      <td>0.129499</td>\n",
       "      <td>0.158682</td>\n",
       "      <td>-0.144962</td>\n",
       "      <td>-0.119584</td>\n",
       "      <td>-0.376815</td>\n",
       "      <td>-0.061156</td>\n",
       "      <td>-0.309046</td>\n",
       "      <td>-0.192605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081260</td>\n",
       "      <td>-0.026421</td>\n",
       "      <td>0.270477</td>\n",
       "      <td>0.034893</td>\n",
       "      <td>-0.121317</td>\n",
       "      <td>0.016192</td>\n",
       "      <td>-0.099173</td>\n",
       "      <td>0.267839</td>\n",
       "      <td>0.076900</td>\n",
       "      <td>0.021036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.105548</td>\n",
       "      <td>-0.297572</td>\n",
       "      <td>0.283156</td>\n",
       "      <td>0.094196</td>\n",
       "      <td>-0.200185</td>\n",
       "      <td>-0.257760</td>\n",
       "      <td>-0.386169</td>\n",
       "      <td>-0.227053</td>\n",
       "      <td>-0.296869</td>\n",
       "      <td>-0.250557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200739</td>\n",
       "      <td>-0.215903</td>\n",
       "      <td>0.135525</td>\n",
       "      <td>0.110088</td>\n",
       "      <td>-0.083183</td>\n",
       "      <td>0.152599</td>\n",
       "      <td>-0.106962</td>\n",
       "      <td>0.275648</td>\n",
       "      <td>0.057450</td>\n",
       "      <td>-0.105681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.157594</td>\n",
       "      <td>-0.283812</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>0.097877</td>\n",
       "      <td>-0.119696</td>\n",
       "      <td>-0.209293</td>\n",
       "      <td>-0.171649</td>\n",
       "      <td>-0.077708</td>\n",
       "      <td>-0.226576</td>\n",
       "      <td>-0.209785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100324</td>\n",
       "      <td>-0.048563</td>\n",
       "      <td>0.223581</td>\n",
       "      <td>0.080459</td>\n",
       "      <td>0.028760</td>\n",
       "      <td>0.165130</td>\n",
       "      <td>-0.159743</td>\n",
       "      <td>0.130813</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>-0.072154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.139515</td>\n",
       "      <td>-0.377321</td>\n",
       "      <td>0.355312</td>\n",
       "      <td>0.040737</td>\n",
       "      <td>-0.139365</td>\n",
       "      <td>-0.201623</td>\n",
       "      <td>-0.140794</td>\n",
       "      <td>-0.036684</td>\n",
       "      <td>-0.036548</td>\n",
       "      <td>-0.328961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232108</td>\n",
       "      <td>-0.179019</td>\n",
       "      <td>0.247628</td>\n",
       "      <td>0.051751</td>\n",
       "      <td>0.140424</td>\n",
       "      <td>0.221385</td>\n",
       "      <td>-0.023135</td>\n",
       "      <td>0.137076</td>\n",
       "      <td>0.110598</td>\n",
       "      <td>-0.153282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.086690</td>\n",
       "      <td>-0.319152</td>\n",
       "      <td>0.358505</td>\n",
       "      <td>0.039078</td>\n",
       "      <td>-0.055743</td>\n",
       "      <td>-0.244824</td>\n",
       "      <td>-0.158496</td>\n",
       "      <td>-0.033350</td>\n",
       "      <td>-0.060453</td>\n",
       "      <td>-0.305937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173713</td>\n",
       "      <td>-0.130920</td>\n",
       "      <td>0.236989</td>\n",
       "      <td>0.067640</td>\n",
       "      <td>0.104986</td>\n",
       "      <td>0.173544</td>\n",
       "      <td>-0.025151</td>\n",
       "      <td>0.131542</td>\n",
       "      <td>0.138977</td>\n",
       "      <td>-0.122601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.027166</td>\n",
       "      <td>-0.251384</td>\n",
       "      <td>0.212212</td>\n",
       "      <td>0.049679</td>\n",
       "      <td>0.017031</td>\n",
       "      <td>-0.113902</td>\n",
       "      <td>-0.222896</td>\n",
       "      <td>-0.097339</td>\n",
       "      <td>-0.168661</td>\n",
       "      <td>-0.223605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015927</td>\n",
       "      <td>0.007578</td>\n",
       "      <td>0.189736</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>-0.079078</td>\n",
       "      <td>-0.008939</td>\n",
       "      <td>-0.037244</td>\n",
       "      <td>0.135518</td>\n",
       "      <td>0.101680</td>\n",
       "      <td>-0.161392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.057757</td>\n",
       "      <td>-0.064144</td>\n",
       "      <td>0.137599</td>\n",
       "      <td>0.012054</td>\n",
       "      <td>0.255869</td>\n",
       "      <td>-0.052690</td>\n",
       "      <td>-0.158635</td>\n",
       "      <td>0.030013</td>\n",
       "      <td>-0.076252</td>\n",
       "      <td>-0.076494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051765</td>\n",
       "      <td>-0.099567</td>\n",
       "      <td>0.204030</td>\n",
       "      <td>-0.005151</td>\n",
       "      <td>-0.029459</td>\n",
       "      <td>-0.228094</td>\n",
       "      <td>0.042326</td>\n",
       "      <td>0.236520</td>\n",
       "      <td>0.182728</td>\n",
       "      <td>-0.148048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.033156</td>\n",
       "      <td>-0.215578</td>\n",
       "      <td>0.465457</td>\n",
       "      <td>0.207164</td>\n",
       "      <td>-0.023271</td>\n",
       "      <td>-0.194054</td>\n",
       "      <td>-0.264121</td>\n",
       "      <td>-0.135186</td>\n",
       "      <td>-0.257206</td>\n",
       "      <td>-0.121328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119029</td>\n",
       "      <td>-0.043766</td>\n",
       "      <td>0.173263</td>\n",
       "      <td>0.070686</td>\n",
       "      <td>-0.087499</td>\n",
       "      <td>0.199291</td>\n",
       "      <td>0.118810</td>\n",
       "      <td>0.147331</td>\n",
       "      <td>0.293618</td>\n",
       "      <td>0.207512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.098067</td>\n",
       "      <td>-0.260682</td>\n",
       "      <td>0.469407</td>\n",
       "      <td>0.221693</td>\n",
       "      <td>-0.045349</td>\n",
       "      <td>0.079533</td>\n",
       "      <td>-0.219020</td>\n",
       "      <td>-0.167237</td>\n",
       "      <td>-0.100364</td>\n",
       "      <td>-0.121049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280442</td>\n",
       "      <td>-0.117627</td>\n",
       "      <td>0.238030</td>\n",
       "      <td>-0.096276</td>\n",
       "      <td>-0.011656</td>\n",
       "      <td>0.089320</td>\n",
       "      <td>0.076170</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.342377</td>\n",
       "      <td>0.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.046527</td>\n",
       "      <td>-0.367255</td>\n",
       "      <td>0.235170</td>\n",
       "      <td>0.039286</td>\n",
       "      <td>0.078672</td>\n",
       "      <td>-0.074001</td>\n",
       "      <td>-0.233394</td>\n",
       "      <td>-0.072252</td>\n",
       "      <td>-0.316420</td>\n",
       "      <td>-0.161928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205672</td>\n",
       "      <td>-0.100162</td>\n",
       "      <td>0.349954</td>\n",
       "      <td>0.075482</td>\n",
       "      <td>-0.105212</td>\n",
       "      <td>0.213139</td>\n",
       "      <td>0.150182</td>\n",
       "      <td>0.192745</td>\n",
       "      <td>0.186867</td>\n",
       "      <td>0.144435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.045304</td>\n",
       "      <td>-0.388473</td>\n",
       "      <td>0.337396</td>\n",
       "      <td>0.152281</td>\n",
       "      <td>0.030535</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>-0.164285</td>\n",
       "      <td>-0.157114</td>\n",
       "      <td>-0.188474</td>\n",
       "      <td>-0.081230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110707</td>\n",
       "      <td>-0.065185</td>\n",
       "      <td>0.285675</td>\n",
       "      <td>0.141813</td>\n",
       "      <td>-0.054493</td>\n",
       "      <td>0.106414</td>\n",
       "      <td>0.225336</td>\n",
       "      <td>0.219161</td>\n",
       "      <td>0.229806</td>\n",
       "      <td>0.080687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.012437</td>\n",
       "      <td>-0.254215</td>\n",
       "      <td>0.364972</td>\n",
       "      <td>0.124824</td>\n",
       "      <td>0.034491</td>\n",
       "      <td>-0.022199</td>\n",
       "      <td>-0.208155</td>\n",
       "      <td>-0.127254</td>\n",
       "      <td>-0.283429</td>\n",
       "      <td>-0.115788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162825</td>\n",
       "      <td>0.143321</td>\n",
       "      <td>0.231700</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>-0.086177</td>\n",
       "      <td>0.082023</td>\n",
       "      <td>0.155922</td>\n",
       "      <td>0.161441</td>\n",
       "      <td>0.264584</td>\n",
       "      <td>0.109275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.009216</td>\n",
       "      <td>-0.269003</td>\n",
       "      <td>0.289657</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.042730</td>\n",
       "      <td>0.161211</td>\n",
       "      <td>-0.199333</td>\n",
       "      <td>-0.200938</td>\n",
       "      <td>-0.124467</td>\n",
       "      <td>-0.121710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033981</td>\n",
       "      <td>0.098107</td>\n",
       "      <td>0.268527</td>\n",
       "      <td>-0.022872</td>\n",
       "      <td>-0.167894</td>\n",
       "      <td>-0.064206</td>\n",
       "      <td>0.190912</td>\n",
       "      <td>0.254535</td>\n",
       "      <td>0.159171</td>\n",
       "      <td>0.010142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.038626</td>\n",
       "      <td>-0.276576</td>\n",
       "      <td>0.293291</td>\n",
       "      <td>0.121479</td>\n",
       "      <td>0.123692</td>\n",
       "      <td>-0.226555</td>\n",
       "      <td>-0.145863</td>\n",
       "      <td>-0.102684</td>\n",
       "      <td>-0.098124</td>\n",
       "      <td>-0.248570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124811</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>0.233774</td>\n",
       "      <td>0.097039</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.159493</td>\n",
       "      <td>0.205918</td>\n",
       "      <td>0.122443</td>\n",
       "      <td>0.205067</td>\n",
       "      <td>0.087198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.080303</td>\n",
       "      <td>-0.536249</td>\n",
       "      <td>0.285948</td>\n",
       "      <td>0.196971</td>\n",
       "      <td>0.066045</td>\n",
       "      <td>-0.087765</td>\n",
       "      <td>-0.176099</td>\n",
       "      <td>-0.089142</td>\n",
       "      <td>-0.156723</td>\n",
       "      <td>-0.296509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227888</td>\n",
       "      <td>-0.059550</td>\n",
       "      <td>0.161833</td>\n",
       "      <td>0.129777</td>\n",
       "      <td>0.058173</td>\n",
       "      <td>0.334280</td>\n",
       "      <td>0.230697</td>\n",
       "      <td>0.235863</td>\n",
       "      <td>0.145281</td>\n",
       "      <td>0.073007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.085556</td>\n",
       "      <td>-0.409314</td>\n",
       "      <td>0.421758</td>\n",
       "      <td>0.183130</td>\n",
       "      <td>0.108903</td>\n",
       "      <td>-0.093976</td>\n",
       "      <td>-0.168773</td>\n",
       "      <td>-0.095474</td>\n",
       "      <td>-0.002262</td>\n",
       "      <td>-0.170754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113649</td>\n",
       "      <td>-0.059510</td>\n",
       "      <td>0.244980</td>\n",
       "      <td>0.064142</td>\n",
       "      <td>0.097302</td>\n",
       "      <td>0.151642</td>\n",
       "      <td>0.272338</td>\n",
       "      <td>0.097772</td>\n",
       "      <td>0.280175</td>\n",
       "      <td>0.103907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 320 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1         2         3         4         5         6         7    \\\n",
       "0                                                                          \n",
       "0   0.064718 -0.288770  0.129499  0.158682 -0.144962 -0.119584 -0.376815   \n",
       "1   0.105548 -0.297572  0.283156  0.094196 -0.200185 -0.257760 -0.386169   \n",
       "2   0.157594 -0.283812  0.344400  0.097877 -0.119696 -0.209293 -0.171649   \n",
       "3   0.139515 -0.377321  0.355312  0.040737 -0.139365 -0.201623 -0.140794   \n",
       "4   0.086690 -0.319152  0.358505  0.039078 -0.055743 -0.244824 -0.158496   \n",
       "5  -0.027166 -0.251384  0.212212  0.049679  0.017031 -0.113902 -0.222896   \n",
       "6   0.057757 -0.064144  0.137599  0.012054  0.255869 -0.052690 -0.158635   \n",
       "7   0.033156 -0.215578  0.465457  0.207164 -0.023271 -0.194054 -0.264121   \n",
       "8   0.098067 -0.260682  0.469407  0.221693 -0.045349  0.079533 -0.219020   \n",
       "9   0.046527 -0.367255  0.235170  0.039286  0.078672 -0.074001 -0.233394   \n",
       "10  0.045304 -0.388473  0.337396  0.152281  0.030535  0.000894 -0.164285   \n",
       "11  0.012437 -0.254215  0.364972  0.124824  0.034491 -0.022199 -0.208155   \n",
       "12  0.009216 -0.269003  0.289657  0.003583  0.042730  0.161211 -0.199333   \n",
       "13  0.038626 -0.276576  0.293291  0.121479  0.123692 -0.226555 -0.145863   \n",
       "14 -0.080303 -0.536249  0.285948  0.196971  0.066045 -0.087765 -0.176099   \n",
       "15  0.085556 -0.409314  0.421758  0.183130  0.108903 -0.093976 -0.168773   \n",
       "\n",
       "         8         9         10   ...       311       312       313       314  \\\n",
       "0                                 ...                                           \n",
       "0  -0.061156 -0.309046 -0.192605  ...  0.081260 -0.026421  0.270477  0.034893   \n",
       "1  -0.227053 -0.296869 -0.250557  ...  0.200739 -0.215903  0.135525  0.110088   \n",
       "2  -0.077708 -0.226576 -0.209785  ...  0.100324 -0.048563  0.223581  0.080459   \n",
       "3  -0.036684 -0.036548 -0.328961  ...  0.232108 -0.179019  0.247628  0.051751   \n",
       "4  -0.033350 -0.060453 -0.305937  ...  0.173713 -0.130920  0.236989  0.067640   \n",
       "5  -0.097339 -0.168661 -0.223605  ...  0.015927  0.007578  0.189736  0.025206   \n",
       "6   0.030013 -0.076252 -0.076494  ... -0.051765 -0.099567  0.204030 -0.005151   \n",
       "7  -0.135186 -0.257206 -0.121328  ...  0.119029 -0.043766  0.173263  0.070686   \n",
       "8  -0.167237 -0.100364 -0.121049  ...  0.280442 -0.117627  0.238030 -0.096276   \n",
       "9  -0.072252 -0.316420 -0.161928  ...  0.205672 -0.100162  0.349954  0.075482   \n",
       "10 -0.157114 -0.188474 -0.081230  ...  0.110707 -0.065185  0.285675  0.141813   \n",
       "11 -0.127254 -0.283429 -0.115788  ...  0.162825  0.143321  0.231700  0.002041   \n",
       "12 -0.200938 -0.124467 -0.121710  ... -0.033981  0.098107  0.268527 -0.022872   \n",
       "13 -0.102684 -0.098124 -0.248570  ...  0.124811  0.020725  0.233774  0.097039   \n",
       "14 -0.089142 -0.156723 -0.296509  ...  0.227888 -0.059550  0.161833  0.129777   \n",
       "15 -0.095474 -0.002262 -0.170754  ...  0.113649 -0.059510  0.244980  0.064142   \n",
       "\n",
       "         315       316       317       318       319       320  \n",
       "0                                                               \n",
       "0  -0.121317  0.016192 -0.099173  0.267839  0.076900  0.021036  \n",
       "1  -0.083183  0.152599 -0.106962  0.275648  0.057450 -0.105681  \n",
       "2   0.028760  0.165130 -0.159743  0.130813  0.086900 -0.072154  \n",
       "3   0.140424  0.221385 -0.023135  0.137076  0.110598 -0.153282  \n",
       "4   0.104986  0.173544 -0.025151  0.131542  0.138977 -0.122601  \n",
       "5  -0.079078 -0.008939 -0.037244  0.135518  0.101680 -0.161392  \n",
       "6  -0.029459 -0.228094  0.042326  0.236520  0.182728 -0.148048  \n",
       "7  -0.087499  0.199291  0.118810  0.147331  0.293618  0.207512  \n",
       "8  -0.011656  0.089320  0.076170  0.006462  0.342377  0.031300  \n",
       "9  -0.105212  0.213139  0.150182  0.192745  0.186867  0.144435  \n",
       "10 -0.054493  0.106414  0.225336  0.219161  0.229806  0.080687  \n",
       "11 -0.086177  0.082023  0.155922  0.161441  0.264584  0.109275  \n",
       "12 -0.167894 -0.064206  0.190912  0.254535  0.159171  0.010142  \n",
       "13  0.037391  0.159493  0.205918  0.122443  0.205067  0.087198  \n",
       "14  0.058173  0.334280  0.230697  0.235863  0.145281  0.073007  \n",
       "15  0.097302  0.151642  0.272338  0.097772  0.280175  0.103907  \n",
       "\n",
       "[16 rows x 320 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6959784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VHH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VHHANEN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VNPHDHQN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LVNPHDHQN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLPHHADADY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LQSGDALRVPSGTTYY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VVKL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IVF</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FSL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SVPQPK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>EVPKA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PLAQPA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LLNPT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HLPLP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>VLPIPQ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   1  2\n",
       "0                      \n",
       "0                AHK  1\n",
       "1                VHH  1\n",
       "2            VHHANEN  1\n",
       "3           VNPHDHQN  1\n",
       "4          LVNPHDHQN  1\n",
       "5         LLPHHADADY  1\n",
       "6   LQSGDALRVPSGTTYY  1\n",
       "7               VVKL  1\n",
       "8                IVF  0\n",
       "9                FSL  0\n",
       "10            SVPQPK  0\n",
       "11             EVPKA  0\n",
       "12            PLAQPA  0\n",
       "13             LLNPT  0\n",
       "14             HLPLP  0\n",
       "15            VLPIPQ  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_out_Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d9b4563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b580e2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877, 320)\n",
      "(877,)\n",
      "310\n",
      "567\n"
     ]
    }
   ],
   "source": [
    "print(X_new.shape)\n",
    "print(y_new.shape)\n",
    "print(np.count_nonzero(y_new==0))\n",
    "print(np.count_nonzero(y_new==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1462f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from self_function import evaluation as eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3629d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset splitting \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_whole, X_ind_test, y_train_whole, y_ind_test =  train_test_split(X_new, y_new, test_size=0.2, random_state=2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ee43d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About model development\n",
      "Best Parameters:{'C': 6}\n",
      "Best cross_validation socre:0.943\n",
      "Test set score:0.95\n",
      "Best_estimator:\n",
      "LogisticRegression(C=6, class_weight='balanced', max_iter=5000, solver='saga')\n",
      "BACC =  0.902 ± 0.05\n",
      "Recall =  0.852 ± 0.109\n",
      "Precision =  0.912 ± 0.039\n",
      "MCC =  0.816 ± 0.076\n",
      "F1 score =  0.876 ± 0.056\n",
      "ROC_AUC =  0.978 ± 0.011\n",
      "LogisticRegression(C=6, class_weight='balanced', max_iter=5000, solver='saga')\n",
      "Sorted Sequence, Predictions, and Probabilities:\n",
      "VHH 1 1.0 0.9999886303877007\n",
      "HLPLP 0 1.0 0.9984982795195666\n",
      "LLPHHADADY 1 1.0 0.9984367439016377\n",
      "VNPHDHQN 1 1.0 0.9945188965708213\n",
      "LVNPHDHQN 1 1.0 0.9894352229763748\n",
      "LQSGDALRVPSGTTYY 1 1.0 0.9447457744923551\n",
      "VHHANEN 1 1.0 0.905066695939048\n",
      "AHK 1 1.0 0.7653020201508623\n",
      "PLAQPA 0 0.0 -\n",
      "VLPIPQ 0 0.0 -\n",
      "SVPQPK 0 0.0 -\n",
      "LLNPT 0 0.0 -\n",
      "VVKL 1 0.0 -\n",
      "EVPKA 0 0.0 -\n",
      "IVF 0 0.0 -\n",
      "FSL 0 0.0 -\n"
     ]
    }
   ],
   "source": [
    "# logistic regresion l2 penality and class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = LogisticRegression(class_weight='balanced',penalty= 'l2', solver= 'saga', max_iter = 5000)\n",
    "param_grid = {'C':[0.5,1,1.5,2,2.5,4,6,8,13,14,15,16,20,30,40, 50,60,70, 80, 90 ,100,125]}\n",
    "grid_search = GridSearchCV(clf,param_grid,cv=10,n_jobs=-1)\n",
    "grid_search.fit(X_train_whole,y_train_whole)\n",
    "best_model_reg = grid_search.best_estimator_\n",
    "y_pred = best_model_reg.predict(X_out)\n",
    "\n",
    "\n",
    "\n",
    "#print the model's parameters and validation score\n",
    "print(\"About model development\")\n",
    "print(\"Best Parameters:{}\".format(grid_search.best_params_))\n",
    "print(\"Best cross_validation socre:{:.3f}\".format(grid_search.best_score_))\n",
    "print(\"Test set score:{:.2f}\".format(grid_search.score(X_ind_test,y_ind_test)))\n",
    "print(\"Best_estimator:\\n{}\".format(grid_search.best_estimator_))\n",
    "\n",
    "#model performance evaluation:BACC & recall & accuracy & MCC & f1 score& roc_auc\n",
    "evaluation = eva(best_model_reg,X_ind_test,y_ind_test)\n",
    "print(evaluation)\n",
    "\n",
    "\n",
    "\n",
    "# get the probability of class 1\n",
    "y_pred_prob = best_model_reg.predict_proba(X_out)[:, 1]\n",
    "\n",
    "# 根据排序后的索引获取预测结果和概率\n",
    "sorted_indices = np.argsort(y_pred_prob)[::-1]\n",
    "sorted_y_pred = y_pred[sorted_indices]\n",
    "sorted_prob = y_pred_prob[sorted_indices]\n",
    "\n",
    "sorted_sequence = X_out_Sequence.iloc[sorted_indices]\n",
    "\n",
    "# 将预测结果为0的序列移到最后\n",
    "zero_indices = np.where(sorted_y_pred == 0)[0]\n",
    "non_zero_indices = np.where(sorted_y_pred != 0)[0]\n",
    "sorted_sequence = pd.concat([sorted_sequence.iloc[non_zero_indices], sorted_sequence.iloc[zero_indices]])\n",
    "sorted_y_pred = np.concatenate((sorted_y_pred[non_zero_indices], sorted_y_pred[zero_indices]))\n",
    "sorted_prob = np.concatenate((sorted_prob[non_zero_indices], sorted_prob[zero_indices]))\n",
    "\n",
    "# 输出结果\n",
    "output = np.column_stack((sorted_sequence, sorted_y_pred, sorted_prob))\n",
    "\n",
    "print(\"Sorted Sequence, Predictions, and Probabilities:\")\n",
    "for item in output:\n",
    "    if item[2] == 0:\n",
    "        print(item[0], item[1], item[2],'-')\n",
    "    else:\n",
    "        print(item[0], item[1], item[2],item[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11589fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACC =  0.943 ± 0.019\n",
      "Recall =  0.927 ± 0.035\n",
      "Precision =  0.926 ± 0.034\n",
      "MCC =  0.886 ± 0.036\n",
      "F1 score =  0.926 ± 0.023\n",
      "ROC_AUC =  0.991 ± 0.004\n",
      "LogisticRegression(C=8, max_iter=5000)\n",
      "About model development\n",
      "Best Parameters:{'C': 8}\n",
      "Best cross_validation socre:0.946\n",
      "Test set score:0.99\n",
      "Best_estimator:\n",
      "LogisticRegression(C=8, max_iter=5000)\n",
      "Sorted Sequence, Predictions, and Probabilities:\n",
      "VHH 1 1.0 0.9999999012515552\n",
      "LLPHHADADY 1 1.0 0.9999799114794801\n",
      "HLPLP 0 1.0 0.9998916539560257\n",
      "VNPHDHQN 1 1.0 0.9985752878366545\n",
      "LVNPHDHQN 1 1.0 0.9972415132727104\n",
      "LQSGDALRVPSGTTYY 1 1.0 0.9859631602295487\n",
      "VHHANEN 1 1.0 0.9763854332552752\n",
      "AHK 1 1.0 0.968172573483699\n",
      "VLPIPQ 0 0.0 -\n",
      "PLAQPA 0 0.0 -\n",
      "SVPQPK 0 0.0 -\n",
      "LLNPT 0 0.0 -\n",
      "EVPKA 0 0.0 -\n",
      "VVKL 1 0.0 -\n",
      "FSL 0 0.0 -\n",
      "IVF 0 0.0 -\n"
     ]
    }
   ],
   "source": [
    "# logistic regresion without penality and solver\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = LogisticRegression(max_iter = 5000)\n",
    "param_grid = {'C':[0.5,1,1.5,2,2.5,4,6,8,13,14,15,16,20,30,40, 50,60,70, 80, 90 ,100,125, 500, 1000,2000,4000,10000]}\n",
    "grid_search = GridSearchCV(clf,param_grid,cv=10,n_jobs=-1)\n",
    "grid_search.fit(X_train_whole,y_train_whole)\n",
    "best_model_reg = grid_search.best_estimator_\n",
    "y_pred = best_model_reg.predict(X_out)\n",
    "#model performance evaluation:BACC & recall & accuracy & MCC & f1 score& roc_auc\n",
    "\n",
    "evaluation = eva(best_model_reg,X_new,y_new)\n",
    "print(evaluation)\n",
    "\n",
    "#print the model's parameters and validation score\n",
    "print(\"About model development\")\n",
    "print(\"Best Parameters:{}\".format(grid_search.best_params_))\n",
    "print(\"Best cross_validation socre:{:.3f}\".format(grid_search.best_score_))\n",
    "print(\"Test set score:{:.2f}\".format(grid_search.score(X_ind_test,y_ind_test)))\n",
    "print(\"Best_estimator:\\n{}\".format(grid_search.best_estimator_))\n",
    "\n",
    "# get the probability of class 1\n",
    "y_pred_prob = best_model_reg.predict_proba(X_out)[:, 1]\n",
    "\n",
    "# 根据排序后的索引获取预测结果和概率\n",
    "sorted_indices = np.argsort(y_pred_prob)[::-1]\n",
    "sorted_y_pred = y_pred[sorted_indices]\n",
    "sorted_prob = y_pred_prob[sorted_indices]\n",
    "\n",
    "sorted_sequence = X_out_Sequence.iloc[sorted_indices]\n",
    "\n",
    "# 将预测结果为0的序列移到最后\n",
    "zero_indices = np.where(sorted_y_pred == 0)[0]\n",
    "non_zero_indices = np.where(sorted_y_pred != 0)[0]\n",
    "sorted_sequence = pd.concat([sorted_sequence.iloc[non_zero_indices], sorted_sequence.iloc[zero_indices]])\n",
    "sorted_y_pred = np.concatenate((sorted_y_pred[non_zero_indices], sorted_y_pred[zero_indices]))\n",
    "sorted_prob = np.concatenate((sorted_prob[non_zero_indices], sorted_prob[zero_indices]))\n",
    "\n",
    "# 输出结果\n",
    "output = np.column_stack((sorted_sequence, sorted_y_pred, sorted_prob))\n",
    "\n",
    "print(\"Sorted Sequence, Predictions, and Probabilities:\")\n",
    "for item in output:\n",
    "    if item[2] == 0:\n",
    "        print(item[0], item[1], item[2],'-')\n",
    "    else:\n",
    "        print(item[0], item[1], item[2],item[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f831c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACC =  0.917 ± 0.017\n",
      "Recall =  0.94 ± 0.028\n",
      "Precision =  0.791 ± 0.04\n",
      "MCC =  0.798 ± 0.037\n",
      "F1 score =  0.858 ± 0.028\n",
      "ROC_AUC =  0.974 ± 0.009\n",
      "RandomForestClassifier(max_depth=5, n_estimators=160, n_jobs=-1)\n",
      "About model development\n",
      "Best Parameters:{'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 160}\n",
      "Best cross_validation socre:0.909\n",
      "Test set score:0.98\n",
      "Best_estimator:\n",
      "RandomForestClassifier(max_depth=5, n_estimators=160, n_jobs=-1)\n",
      "Sorted Sequence, Predictions, and Probabilities:\n",
      "VHH 1 1.0 0.9546838241111713\n",
      "HLPLP 0 1.0 0.9526656085183088\n",
      "LVNPHDHQN 1 1.0 0.9169252622130528\n",
      "VNPHDHQN 1 1.0 0.9115080383335222\n",
      "LLPHHADADY 1 1.0 0.9084420029130312\n",
      "VHHANEN 1 1.0 0.8305742745235631\n",
      "LQSGDALRVPSGTTYY 1 1.0 0.7023571790742082\n",
      "AHK 1 1.0 0.6962706991384162\n",
      "VLPIPQ 0 1.0 0.6854491922147996\n",
      "SVPQPK 0 1.0 0.537211743700745\n",
      "LLNPT 0 0.0 -\n",
      "PLAQPA 0 0.0 -\n",
      "FSL 0 0.0 -\n",
      "IVF 0 0.0 -\n",
      "EVPKA 0 0.0 -\n",
      "VVKL 1 0.0 -\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "param_grid = {'n_estimators': [80, 160, 320, 480, 640, 1280,2000],\n",
    "            'max_depth': [1, 2, 3, 4,5],\n",
    "            'max_features' :['log2', 'sqrt']}\n",
    "grid_search = GridSearchCV(clf,param_grid,cv=10,n_jobs=-1)\n",
    "grid_search.fit(X_train_whole,y_train_whole)\n",
    "best_model_reg = grid_search.best_estimator_\n",
    "y_pred = best_model_reg.predict(X_out)\n",
    "#评价 准确率 recall 精确率 rocauc f1 mcc\n",
    "\n",
    "evaluation = eva(best_model_reg,X_new,y_new)\n",
    "print(evaluation)\n",
    "\n",
    "#print the model's parameters and validation score\n",
    "print(\"About model development\")\n",
    "print(\"Best Parameters:{}\".format(grid_search.best_params_))\n",
    "print(\"Best cross_validation socre:{:.3f}\".format(grid_search.best_score_))\n",
    "print(\"Test set score:{:.2f}\".format(grid_search.score(X_ind_test,y_ind_test)))\n",
    "print(\"Best_estimator:\\n{}\".format(grid_search.best_estimator_))\n",
    "\n",
    "# get the probability of class 1\n",
    "y_pred_prob = best_model_reg.predict_proba(X_out)[:, 1]\n",
    "\n",
    "# 根据排序后的索引获取预测结果和概率\n",
    "sorted_indices = np.argsort(y_pred_prob)[::-1]\n",
    "sorted_y_pred = y_pred[sorted_indices]\n",
    "sorted_prob = y_pred_prob[sorted_indices]\n",
    "\n",
    "sorted_sequence = X_out_Sequence.iloc[sorted_indices]\n",
    "\n",
    "# 将预测结果为0的序列移到最后\n",
    "zero_indices = np.where(sorted_y_pred == 0)[0]\n",
    "non_zero_indices = np.where(sorted_y_pred != 0)[0]\n",
    "sorted_sequence = pd.concat([sorted_sequence.iloc[non_zero_indices], sorted_sequence.iloc[zero_indices]])\n",
    "sorted_y_pred = np.concatenate((sorted_y_pred[non_zero_indices], sorted_y_pred[zero_indices]))\n",
    "sorted_prob = np.concatenate((sorted_prob[non_zero_indices], sorted_prob[zero_indices]))\n",
    "\n",
    "# 输出结果\n",
    "output = np.column_stack((sorted_sequence, sorted_y_pred, sorted_prob))\n",
    "\n",
    "print(\"Sorted Sequence, Predictions, and Probabilities:\")\n",
    "for item in output:\n",
    "    if item[2] == 0:\n",
    "        print(item[0], item[1], item[2],'-')\n",
    "    else:\n",
    "        print(item[0], item[1], item[2],item[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "114b1e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACC =  0.909 ± 0.021\n",
      "Recall =  0.92 ± 0.04\n",
      "Precision =  0.802 ± 0.049\n",
      "MCC =  0.791 ± 0.043\n",
      "F1 score =  0.856 ± 0.031\n",
      "ROC_AUC =  0.973 ± 0.009\n",
      "KNeighborsClassifier(leaf_size=10, n_jobs=-1, n_neighbors=23,\n",
      "                     weights='distance')\n",
      "About model development\n",
      "Best Parameters:{'algorithm': 'auto', 'leaf_size': 10, 'n_neighbors': 23, 'weights': 'distance'}\n",
      "Best cross_validation socre:0.900\n",
      "Test set score:0.99\n",
      "Best_estimator:\n",
      "KNeighborsClassifier(leaf_size=10, n_jobs=-1, n_neighbors=23,\n",
      "                     weights='distance')\n",
      "Sorted Sequence, Predictions, and Probabilities:\n",
      "HLPLP 0 1.0 1.0\n",
      "VHH 1 1.0 1.0\n",
      "VNPHDHQN 1 1.0 0.9595753592560136\n",
      "LLPHHADADY 1 1.0 0.8790116694487203\n",
      "LVNPHDHQN 1 1.0 0.8384298904551504\n",
      "VHHANEN 1 1.0 0.7829840164383055\n",
      "AHK 1 1.0 0.7809744968939138\n",
      "LQSGDALRVPSGTTYY 1 1.0 0.6532848633994651\n",
      "VLPIPQ 0 1.0 0.6422956731194804\n",
      "SVPQPK 0 1.0 0.5842357812413839\n",
      "LLNPT 0 0.0 -\n",
      "PLAQPA 0 0.0 -\n",
      "EVPKA 0 0.0 -\n",
      "FSL 0 0.0 -\n",
      "VVKL 1 0.0 -\n",
      "IVF 0 0.0 -\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = KNeighborsClassifier(n_jobs=-1)\n",
    "param_grid = {'n_neighbors' : [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43],'weights' : ['uniform', 'distance'],\n",
    "              'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "              'leaf_size' : [10, 20, 30, 40, 50, 60, 70, 80]}\n",
    "grid_search = GridSearchCV(clf,param_grid,cv=10,n_jobs=-1)\n",
    "grid_search.fit(X_train_whole,y_train_whole)\n",
    "best_model_reg = grid_search.best_estimator_\n",
    "y_pred = best_model_reg.predict(X_out)\n",
    "#评价 准确率 recall 精确率 rocauc f1 mcc\n",
    "\n",
    "evaluation = eva(best_model_reg,X_new,y_new)\n",
    "print(evaluation)\n",
    "\n",
    "#print the model's parameters and validation score\n",
    "print(\"About model development\")\n",
    "print(\"Best Parameters:{}\".format(grid_search.best_params_))\n",
    "print(\"Best cross_validation socre:{:.3f}\".format(grid_search.best_score_))\n",
    "print(\"Test set score:{:.2f}\".format(grid_search.score(X_ind_test,y_ind_test)))\n",
    "print(\"Best_estimator:\\n{}\".format(grid_search.best_estimator_))\n",
    "\n",
    "# get the probability of class 1\n",
    "y_pred_prob = best_model_reg.predict_proba(X_out)[:, 1]\n",
    "\n",
    "# 根据排序后的索引获取预测结果和概率\n",
    "sorted_indices = np.argsort(y_pred_prob)[::-1]\n",
    "sorted_y_pred = y_pred[sorted_indices]\n",
    "sorted_prob = y_pred_prob[sorted_indices]\n",
    "\n",
    "sorted_sequence = X_out_Sequence.iloc[sorted_indices]\n",
    "\n",
    "# 将预测结果为0的序列移到最后\n",
    "zero_indices = np.where(sorted_y_pred == 0)[0]\n",
    "non_zero_indices = np.where(sorted_y_pred != 0)[0]\n",
    "sorted_sequence = pd.concat([sorted_sequence.iloc[non_zero_indices], sorted_sequence.iloc[zero_indices]])\n",
    "sorted_y_pred = np.concatenate((sorted_y_pred[non_zero_indices], sorted_y_pred[zero_indices]))\n",
    "sorted_prob = np.concatenate((sorted_prob[non_zero_indices], sorted_prob[zero_indices]))\n",
    "\n",
    "# 输出结果\n",
    "output = np.column_stack((sorted_sequence, sorted_y_pred, sorted_prob))\n",
    "\n",
    "print(\"Sorted Sequence, Predictions, and Probabilities:\")\n",
    "for item in output:\n",
    "    if item[2] == 0:\n",
    "        print(item[0], item[1], item[2],'-')\n",
    "    else:\n",
    "        print(item[0], item[1], item[2],item[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b31fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACC =  0.948 ± 0.018\n",
      "Recall =  0.937 ± 0.033\n",
      "Precision =  0.923 ± 0.038\n",
      "MCC =  0.892 ± 0.033\n",
      "F1 score =  0.929 ± 0.022\n",
      "ROC_AUC =  0.989 ± 0.005\n",
      "SVC(C=30, kernel='poly', tol=1e-05)\n",
      "About model development\n",
      "Best Parameters:{'C': 30, 'degree': 3, 'kernel': 'poly', 'tol': 1e-05}\n",
      "Best cross_validation socre:0.950\n",
      "Test set score:0.98\n",
      "Best_estimator:\n",
      "SVC(C=30, kernel='poly', tol=1e-05)\n",
      "Sorted Sequence, Predictions, and Probabilities:\n",
      "VHH 1 1.0 0.9993155497230358\n",
      "LLPHHADADY 1 1.0 0.9917183793396691\n",
      "HLPLP 0 1.0 0.9850024376753495\n",
      "VNPHDHQN 1 1.0 0.9518975245770803\n",
      "LVNPHDHQN 1 1.0 0.942178389788647\n",
      "AHK 1 1.0 0.9059827736426143\n",
      "VHHANEN 1 1.0 0.8647834910628434\n",
      "LQSGDALRVPSGTTYY 1 1.0 0.8121657443688994\n",
      "VLPIPQ 0 0.0 -\n",
      "PLAQPA 0 0.0 -\n",
      "SVPQPK 0 0.0 -\n",
      "LLNPT 0 0.0 -\n",
      "EVPKA 0 0.0 -\n",
      "VVKL 1 0.0 -\n",
      "FSL 0 0.0 -\n",
      "IVF 0 0.0 -\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = SVC()\n",
    "param_grid = {'C':[0.001, 0.01, 0.1,  0.5, 1.1, 1.3, 5, 10, 30, 50, 100, 300],'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              'degree':[1, 3, 5, 7, 9],'tol':[1e-5]}\n",
    "grid_search = GridSearchCV(clf,param_grid,cv=10,n_jobs=-1)\n",
    "grid_search.fit(X_train_whole,y_train_whole)\n",
    "best_model_reg = grid_search.best_estimator_\n",
    "y_pred = best_model_reg.predict(X_out)\n",
    "#评价 准确率 recall 精确率 rocauc f1 mcc\n",
    "\n",
    "evaluation = eva(best_model_reg,X_new,y_new)\n",
    "print(evaluation)\n",
    "\n",
    "#save model\n",
    "import pickle\n",
    "\n",
    "\n",
    "#print the model's parameters and validation score\n",
    "print(\"About model development\")\n",
    "print(\"Best Parameters:{}\".format(grid_search.best_params_))\n",
    "print(\"Best cross_validation socre:{:.3f}\".format(grid_search.best_score_))\n",
    "print(\"Test set score:{:.2f}\".format(grid_search.score(X_ind_test,y_ind_test)))\n",
    "print(\"Best_estimator:\\n{}\".format(grid_search.best_estimator_))\n",
    "\n",
    "# get the probability of class 1\n",
    "decision_values = best_model_reg.decision_function(X_out)\n",
    "\n",
    "\n",
    "y_pred_prob = 1 / (1 + np.exp(-decision_values))\n",
    "\n",
    "# 根据排序后的索引获取预测结果和概率\n",
    "sorted_indices = np.argsort(y_pred_prob)[::-1]\n",
    "sorted_y_pred = y_pred[sorted_indices]\n",
    "sorted_prob = y_pred_prob[sorted_indices]\n",
    "\n",
    "sorted_sequence = X_out_Sequence.iloc[sorted_indices]\n",
    "\n",
    "# 将预测结果为0的序列移到最后\n",
    "zero_indices = np.where(sorted_y_pred == 0)[0]\n",
    "non_zero_indices = np.where(sorted_y_pred != 0)[0]\n",
    "sorted_sequence = pd.concat([sorted_sequence.iloc[non_zero_indices], sorted_sequence.iloc[zero_indices]])\n",
    "sorted_y_pred = np.concatenate((sorted_y_pred[non_zero_indices], sorted_y_pred[zero_indices]))\n",
    "sorted_prob = np.concatenate((sorted_prob[non_zero_indices], sorted_prob[zero_indices]))\n",
    "\n",
    "# 输出结果\n",
    "output = np.column_stack((sorted_sequence, sorted_y_pred, sorted_prob))\n",
    "\n",
    "print(\"Sorted Sequence, Predictions, and Probabilities:\")\n",
    "for item in output:\n",
    "    if item[2] == 0:\n",
    "        print(item[0], item[1], item[2],'-')\n",
    "    else:\n",
    "        print(item[0], item[1], item[2],item[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9858256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACC =  0.933 ± 0.018\n",
      "Recall =  0.919 ± 0.027\n",
      "Precision =  0.901 ± 0.056\n",
      "MCC =  0.861 ± 0.04\n",
      "F1 score =  0.909 ± 0.026\n",
      "ROC_AUC =  0.984 ± 0.009\n",
      "MLPClassifier(early_stopping=True, hidden_layer_sizes=1280, max_iter=3000,\n",
      "              n_iter_no_change=40)\n",
      "About model development\n",
      "Best Parameters:{'hidden_layer_sizes': 1280}\n",
      "Best cross_validation socre:0.936\n",
      "Test set score:0.96\n",
      "Best_estimator:\n",
      "MLPClassifier(early_stopping=True, hidden_layer_sizes=1280, max_iter=3000,\n",
      "              n_iter_no_change=40)\n",
      "Sorted Sequence, Predictions, and Probabilities:\n",
      "VHH 1 1.0 0.999994574516385\n",
      "LLPHHADADY 1 1.0 0.9999424163413002\n",
      "HLPLP 0 1.0 0.9995702071812892\n",
      "VNPHDHQN 1 1.0 0.998234998369792\n",
      "LVNPHDHQN 1 1.0 0.995128284806887\n",
      "LQSGDALRVPSGTTYY 1 1.0 0.9908049435464016\n",
      "VHHANEN 1 1.0 0.9513477274219413\n",
      "AHK 1 1.0 0.9032310610404802\n",
      "PLAQPA 0 1.0 0.8309744899965238\n",
      "VLPIPQ 0 0.0 -\n",
      "SVPQPK 0 0.0 -\n",
      "LLNPT 0 0.0 -\n",
      "EVPKA 0 0.0 -\n",
      "VVKL 1 0.0 -\n",
      "FSL 0 0.0 -\n",
      "IVF 0 0.0 -\n"
     ]
    }
   ],
   "source": [
    "# MLP one layer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = MLPClassifier(early_stopping= True,n_iter_no_change= 40, max_iter = 3000)\n",
    "param_grid = {'hidden_layer_sizes':[1,2,3,4,6,8,12,16, 18, 20, 22, 24,26,28,32, 64, 128, 240, 320, 640,1280]}\n",
    "grid_search = GridSearchCV(clf,param_grid,cv=10,n_jobs=-1)\n",
    "grid_search.fit(X_train_whole,y_train_whole)\n",
    "best_model_reg = grid_search.best_estimator_\n",
    "y_pred = best_model_reg.predict(X_out)\n",
    "#评价 准确率 recall 精确率 rocauc f1 mcc\n",
    "\n",
    "evaluation = eva(best_model_reg,X_new,y_new)\n",
    "print(evaluation)\n",
    "\n",
    "#print the model's parameters and validation score\n",
    "print(\"About model development\")\n",
    "print(\"Best Parameters:{}\".format(grid_search.best_params_))\n",
    "print(\"Best cross_validation socre:{:.3f}\".format(grid_search.best_score_))\n",
    "print(\"Test set score:{:.2f}\".format(grid_search.score(X_ind_test,y_ind_test)))\n",
    "print(\"Best_estimator:\\n{}\".format(grid_search.best_estimator_))\n",
    "\n",
    "# get the probability of class 1\n",
    "y_pred_prob = best_model_reg.predict_proba(X_out)[:, 1]\n",
    "\n",
    "# 根据排序后的索引获取预测结果和概率\n",
    "sorted_indices = np.argsort(y_pred_prob)[::-1]\n",
    "sorted_y_pred = y_pred[sorted_indices]\n",
    "sorted_prob = y_pred_prob[sorted_indices]\n",
    "\n",
    "sorted_sequence = X_out_Sequence.iloc[sorted_indices]\n",
    "\n",
    "# 将预测结果为0的序列移到最后\n",
    "zero_indices = np.where(sorted_y_pred == 0)[0]\n",
    "non_zero_indices = np.where(sorted_y_pred != 0)[0]\n",
    "sorted_sequence = pd.concat([sorted_sequence.iloc[non_zero_indices], sorted_sequence.iloc[zero_indices]])\n",
    "sorted_y_pred = np.concatenate((sorted_y_pred[non_zero_indices], sorted_y_pred[zero_indices]))\n",
    "sorted_prob = np.concatenate((sorted_prob[non_zero_indices], sorted_prob[zero_indices]))\n",
    "\n",
    "# 输出结果\n",
    "output = np.column_stack((sorted_sequence, sorted_y_pred, sorted_prob))\n",
    "\n",
    "print(\"Sorted Sequence, Predictions, and Probabilities:\")\n",
    "for item in output:\n",
    "    if item[2] == 0:\n",
    "        print(item[0], item[1], item[2],'-')\n",
    "    else:\n",
    "        print(item[0], item[1], item[2],item[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69cbfe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About model development\n",
      "Best Parameters:{'hidden_layer_sizes': (320, 24)}\n",
      "Best cross_validation socre:0.946\n",
      "Test set score:0.97\n",
      "Best_estimator:\n",
      "MLPClassifier(early_stopping=True, hidden_layer_sizes=(320, 24), max_iter=3000,\n",
      "              n_iter_no_change=40)\n",
      "BACC =  0.931 ± 0.013\n",
      "Recall =  0.909 ± 0.026\n",
      "Precision =  0.911 ± 0.052\n",
      "MCC =  0.861 ± 0.031\n",
      "F1 score =  0.909 ± 0.022\n",
      "ROC_AUC =  0.986 ± 0.007\n",
      "MLPClassifier(early_stopping=True, hidden_layer_sizes=(320, 24), max_iter=3000,\n",
      "              n_iter_no_change=40)\n",
      "Sorted Sequence, Predictions, and Probabilities:\n",
      "VHH 1 1.0 0.9996853199910621\n",
      "LLPHHADADY 1 1.0 0.9983465269531823\n",
      "HLPLP 0 1.0 0.9976588076032726\n",
      "VNPHDHQN 1 1.0 0.9953220531476388\n",
      "LVNPHDHQN 1 1.0 0.9874789423907772\n",
      "VHHANEN 1 1.0 0.9095486346264774\n",
      "LQSGDALRVPSGTTYY 1 1.0 0.909207472861857\n",
      "AHK 1 1.0 0.6002699493280288\n",
      "VLPIPQ 0 0.0 -\n",
      "PLAQPA 0 0.0 -\n",
      "LLNPT 0 0.0 -\n",
      "SVPQPK 0 0.0 -\n",
      "FSL 0 0.0 -\n",
      "IVF 0 0.0 -\n",
      "EVPKA 0 0.0 -\n",
      "VVKL 1 0.0 -\n"
     ]
    }
   ],
   "source": [
    "# MLP two layer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import itertools\n",
    "\n",
    "hidden_layers_1 = [12,24,32,64,128,256,320,640,1280]\n",
    "hidden_layers_2 = [2,8,12,24,32,64,128,256]\n",
    "combinations = list(itertools.product(hidden_layers_1, hidden_layers_2))\n",
    "hidden_layers_in =  [(x, y) for x, y in combinations]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = MLPClassifier(early_stopping= True,n_iter_no_change= 40, max_iter = 3000)\n",
    "param_grid = {'hidden_layer_sizes':hidden_layers_in }\n",
    "grid_search = GridSearchCV(clf,param_grid,cv=10,n_jobs=-1)\n",
    "grid_search.fit(X_train_whole,y_train_whole)\n",
    "best_model_reg = grid_search.best_estimator_\n",
    "y_pred = best_model_reg.predict(X_out)\n",
    "\n",
    "\n",
    "#print the model's parameters and validation score\n",
    "print(\"About model development\")\n",
    "print(\"Best Parameters:{}\".format(grid_search.best_params_))\n",
    "print(\"Best cross_validation socre:{:.3f}\".format(grid_search.best_score_))\n",
    "print(\"Test set score:{:.2f}\".format(grid_search.score(X_ind_test,y_ind_test)))\n",
    "print(\"Best_estimator:\\n{}\".format(grid_search.best_estimator_))\n",
    "\n",
    "#model performance evaluation:BACC & recall & accuracy & MCC & f1 score& roc_auc\n",
    "evaluation = eva(best_model_reg,X_new,y_new)\n",
    "print(evaluation)\n",
    "\n",
    "\n",
    "\n",
    "# get the probability of class 1\n",
    "y_pred_prob = best_model_reg.predict_proba(X_out)[:, 1]\n",
    "\n",
    "# 根据排序后的索引获取预测结果和概率\n",
    "sorted_indices = np.argsort(y_pred_prob)[::-1]\n",
    "sorted_y_pred = y_pred[sorted_indices]\n",
    "sorted_prob = y_pred_prob[sorted_indices]\n",
    "\n",
    "sorted_sequence = X_out_Sequence.iloc[sorted_indices]\n",
    "\n",
    "# 将预测结果为0的序列移到最后\n",
    "zero_indices = np.where(sorted_y_pred == 0)[0]\n",
    "non_zero_indices = np.where(sorted_y_pred != 0)[0]\n",
    "sorted_sequence = pd.concat([sorted_sequence.iloc[non_zero_indices], sorted_sequence.iloc[zero_indices]])\n",
    "sorted_y_pred = np.concatenate((sorted_y_pred[non_zero_indices], sorted_y_pred[zero_indices]))\n",
    "sorted_prob = np.concatenate((sorted_prob[non_zero_indices], sorted_prob[zero_indices]))\n",
    "\n",
    "# 输出结果\n",
    "output = np.column_stack((sorted_sequence, sorted_y_pred, sorted_prob))\n",
    "\n",
    "print(\"Sorted Sequence, Predictions, and Probabilities:\")\n",
    "for item in output:\n",
    "    if item[2] == 0:\n",
    "        print(item[0], item[1], item[2],'-')\n",
    "    else:\n",
    "        print(item[0], item[1], item[2],item[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1019a8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About model development\n",
      "Best Parameters:{'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 160, 'reg_lambda': 0}\n",
      "Best cross_validation socre:0.936\n",
      "Test set score:0.90\n",
      "Best_estimator:\n",
      "LGBMClassifier(boosting='gbdt', n_estimators=160, n_jobs=-1, objective='binary',\n",
      "               reg_lambda=0, verbose=-1)\n",
      "BACC =  0.935 ± 0.013\n",
      "Recall =  0.935 ± 0.031\n",
      "Precision =  0.878 ± 0.036\n",
      "MCC =  0.858 ± 0.023\n",
      "F1 score =  0.905 ± 0.015\n",
      "ROC_AUC =  0.981 ± 0.007\n",
      "LGBMClassifier(boosting='gbdt', n_estimators=160, n_jobs=-1, objective='binary',\n",
      "               reg_lambda=0, verbose=-1)\n",
      "Sorted Sequence, Predictions, and Probabilities:\n",
      "VHH 1 1.0 0.9999997177084736\n",
      "HLPLP 0 1.0 0.9999996464667034\n",
      "LLPHHADADY 1 1.0 0.9999993701405997\n",
      "VNPHDHQN 1 1.0 0.9999984662867262\n",
      "LVNPHDHQN 1 1.0 0.9999967559387167\n",
      "VHHANEN 1 1.0 0.9999024811693963\n",
      "LQSGDALRVPSGTTYY 1 1.0 0.9997259075634901\n",
      "AHK 1 1.0 0.9997164864913427\n",
      "VLPIPQ 0 0.0 -\n",
      "SVPQPK 0 0.0 -\n",
      "PLAQPA 0 0.0 -\n",
      "LLNPT 0 0.0 -\n",
      "IVF 0 0.0 -\n",
      "VVKL 1 0.0 -\n",
      "FSL 0 0.0 -\n",
      "EVPKA 0 0.0 -\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "clf = lgb.LGBMClassifier(boosting='gbdt', objective='binary',n_jobs=-1,verbose = -1)\n",
    "param_grid = {'learning_rate':[0.001,0.01,0.1,0.2,0.5,1],\n",
    "              'n_estimators':[40,80,160,320,640],\n",
    "              'max_depth':[-1],\n",
    "              'reg_lambda':[0]}\n",
    "grid_search = GridSearchCV(clf,param_grid,cv=10,n_jobs=-1)\n",
    "grid_search.fit(X_train_whole,y_train_whole)\n",
    "best_model_reg = grid_search.best_estimator_\n",
    "y_pred = best_model_reg.predict(X_out)\n",
    "\n",
    "\n",
    "#print the model's parameters and validation score\n",
    "print(\"About model development\")\n",
    "print(\"Best Parameters:{}\".format(grid_search.best_params_))\n",
    "print(\"Best cross_validation socre:{:.3f}\".format(grid_search.best_score_))\n",
    "print(\"Test set score:{:.2f}\".format(grid_search.score(X_ind_test,y_ind_test)))\n",
    "print(\"Best_estimator:\\n{}\".format(grid_search.best_estimator_))\n",
    "\n",
    "#model performance evaluation:BACC & recall & accuracy & MCC & f1 score& roc_auc\n",
    "evaluation = eva(best_model_reg,X_new,y_new)\n",
    "print(evaluation)\n",
    "\n",
    "\n",
    "\n",
    "# get the probability of class 1\n",
    "y_pred_prob = best_model_reg.predict_proba(X_out)[:, 1]\n",
    "\n",
    "# 根据排序后的索引获取预测结果和概率\n",
    "sorted_indices = np.argsort(y_pred_prob)[::-1]\n",
    "sorted_y_pred = y_pred[sorted_indices]\n",
    "sorted_prob = y_pred_prob[sorted_indices]\n",
    "\n",
    "sorted_sequence = X_out_Sequence.iloc[sorted_indices]\n",
    "\n",
    "# 将预测结果为0的序列移到最后\n",
    "zero_indices = np.where(sorted_y_pred == 0)[0]\n",
    "non_zero_indices = np.where(sorted_y_pred != 0)[0]\n",
    "sorted_sequence = pd.concat([sorted_sequence.iloc[non_zero_indices], sorted_sequence.iloc[zero_indices]])\n",
    "sorted_y_pred = np.concatenate((sorted_y_pred[non_zero_indices], sorted_y_pred[zero_indices]))\n",
    "sorted_prob = np.concatenate((sorted_prob[non_zero_indices], sorted_prob[zero_indices]))\n",
    "\n",
    "# 输出结果\n",
    "output = np.column_stack((sorted_sequence, sorted_y_pred, sorted_prob))\n",
    "\n",
    "print(\"Sorted Sequence, Predictions, and Probabilities:\")\n",
    "for item in output:\n",
    "    if item[2] == 0:\n",
    "        print(item[0], item[1], item[2],'-')\n",
    "    else:\n",
    "        print(item[0], item[1], item[2],item[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411013a4",
   "metadata": {},
   "source": [
    "# 可视化降维\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f7b44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf55a84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#after clean umap\n",
    "X=X_new\n",
    "y=y_new\n",
    "embedding = umap.UMAP(n_neighbors=200).fit_transform(X, y=y) # two dimension\n",
    "df = pd.DataFrame()\n",
    "df[\"comp-1\"] = embedding[:,0]\n",
    "df[\"comp-2\"] = embedding[:,1]\n",
    "y_new_label=[]\n",
    "for i in y:\n",
    "    if i == 0:\n",
    "        y_new_label.append('Active')\n",
    "    if i == 1:\n",
    "        y_new_label.append('Inactive')\n",
    "df[\"y\"] = y_new_label\n",
    "graph = sns.scatterplot(data=df, x=\"comp-1\", y=\"comp-2\", hue=y_new_label,\n",
    "                palette='YlOrRd_r', legend='full')\n",
    "graph_for_output = graph.get_figure()\n",
    "graph_for_output.savefig('after_clean_UMAP.png', dpi=300)\n",
    "df.to_excel('after_clean_UMAP.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d68ee",
   "metadata": {},
   "source": [
    "## DeepLearning深度学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efca241f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877, 320)\n",
      "(877,)\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense,BatchNormalization,Dropout\n",
    "from numpy import loadtxt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, matthews_corrcoef\n",
    "import keras\n",
    "\n",
    "# 读取数据\n",
    "data = X_new\n",
    "# 将数据分为特征和标签\n",
    "X = data.iloc[:, 0:].values\n",
    "print(X.shape)\n",
    "y = y_new\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e244a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\engine\\training.py:788 train_step\n        loss = self.compiled_loss(\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\engine\\compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\losses.py:1809 binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\backend.py:5000 binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:245 sigmoid_cross_entropy_with_logits_v2\n        return sigmoid_cross_entropy_with_logits(\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:132 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((None, 2) vs (None, 1))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 在测试集上评估模型\u001b[39;00m\n\u001b[0;32m     32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mD:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    932\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 933\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    936\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    937\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mD:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:759\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph \u001b[38;5;241m=\u001b[39m lifted_initializer_graph\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_deleter \u001b[38;5;241m=\u001b[39m FunctionDeleter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph)\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_stateful_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 759\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_get_concrete_function_internal_garbage_collected(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    760\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    763\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mD:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3066\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 3066\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3067\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mD:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3463\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[0;32m   3460\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 3463\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mprimary[cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[0;32m   3466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32mD:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3298\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3293\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3294\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3295\u001b[0m ]\n\u001b[0;32m   3296\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   3297\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3298\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3300\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3301\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3303\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3306\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3307\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   3310\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   3311\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3312\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3313\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3314\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3315\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mD:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1007\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1005\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1007\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m   1012\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:668\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    665\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    666\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 668\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    669\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mD:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:994\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    993\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m    995\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\engine\\training.py:788 train_step\n        loss = self.compiled_loss(\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\engine\\compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\losses.py:1809 binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\keras\\backend.py:5000 binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:245 sigmoid_cross_entropy_with_logits_v2\n        return sigmoid_cross_entropy_with_logits(\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    D:\\py\\anaconda3\\envs\\tf2.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:132 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((None, 2) vs (None, 1))\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 数据预处理\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "# 构建CNN模型\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=input_shape,padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# 编译模型\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=1)\n",
    "\n",
    "# 在测试集上评估模型\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = np.round(y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "mcc = matthews_corrcoef(y_test, y_pred_binary)\n",
    "print(\"MCC:\", mcc)\n",
    "\n",
    "# 预测新的序列\n",
    "#new_sequence = np.array([your_new_sequence])  # 替换成您的新序列数据\n",
    "#new_sequence = new_sequence.reshape(1, new_sequence.shape[0], 1)\n",
    "#prediction = model.predict(new_sequence)\n",
    "#if prediction > 0.5:\n",
    "   # print('新的序列具有抗氧化性')\n",
    "#else:\n",
    "   # print('新的序列没有抗氧化性')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3d28271",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sequence_data = X_out # 替换成您的新序列数据文件\n",
    "new_sequence = new_sequence_data.iloc[:, 0:].values\n",
    "new_sequence = new_sequence.reshape(new_sequence.shape[0], new_sequence.shape[1], 1)\n",
    "predictions = model.predict(new_sequence)\n",
    "predictions_proba = predictions.flatten()\n",
    "sorted_indices_CNN = np.argsort(predictions_proba)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8530f4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Sequence, Predictions, and Probabilities:\n",
      "[['LWL' 0 1.0]\n",
      " ['LQSGDALRVPSGTTYY' 1 1.0]\n",
      " ['VHH' 1 1.0]\n",
      " ['VHHANEN' 1 1.0]\n",
      " ['VNPHDHQN' 1 1.0]\n",
      " ['LVNPHDHQN' 1 1.0]\n",
      " ['LLPHHADADY' 1 1.0]\n",
      " ['VIPAGYP' 1 1.0]\n",
      " ['AHK' 1 1.0]\n",
      " ['PP' 0 1.0]\n",
      " ['HLPLP' 0 1.0]\n",
      " ['VLPIPQ' 0 0.9335595965385437]\n",
      " ['SVPQPK' 0 1.921911518820707e-07]\n",
      " ['EVPKA' 0 1.1918875486571778e-07]\n",
      " ['LLNPT' 0 6.358715864962505e-08]\n",
      " ['PLAQPA' 0 6.124222551306957e-08]\n",
      " ['FSL' 0 4.531619879344362e-08]\n",
      " ['VKL' 1 5.731187191493348e-10]\n",
      " ['VVKL' 1 5.616606624236908e-10]\n",
      " ['IVF' 0 5.528474900096114e-10]]\n"
     ]
    }
   ],
   "source": [
    "sorted_sequence_CNN = X_out_Sequence.iloc[sorted_indices_CNN]\n",
    "sorted_proba_CNN = predictions_proba[sorted_indices_CNN]\n",
    "output = np.column_stack((sorted_sequence_CNN, sorted_proba_CNN))\n",
    "\n",
    "print(\"Sorted Sequence, Predictions, and Probabilities:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa83731d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 7s 16ms/step - loss: 0.6804 - binary_accuracy: 0.6562\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6338 - binary_accuracy: 0.6562\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5189 - binary_accuracy: 0.7190\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.3342 - binary_accuracy: 0.8816\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.2199 - binary_accuracy: 0.9344\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.1580 - binary_accuracy: 0.9486\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.1514 - binary_accuracy: 0.9558\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.1244 - binary_accuracy: 0.9615\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.1278 - binary_accuracy: 0.9586\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.1237 - binary_accuracy: 0.9515\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.1210 - binary_accuracy: 0.9629\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.1221 - binary_accuracy: 0.9558\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.1380 - binary_accuracy: 0.9501\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.1156 - binary_accuracy: 0.9601\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0752 - binary_accuracy: 0.9672\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0957 - binary_accuracy: 0.9629\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0703 - binary_accuracy: 0.9686\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0653 - binary_accuracy: 0.9715\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0675 - binary_accuracy: 0.9686\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0639 - binary_accuracy: 0.9715\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0540 - binary_accuracy: 0.9757\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0508 - binary_accuracy: 0.9843\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0645 - binary_accuracy: 0.9700\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0714 - binary_accuracy: 0.9715\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0502 - binary_accuracy: 0.9786\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0500 - binary_accuracy: 0.9800\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0549 - binary_accuracy: 0.9743\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0336 - binary_accuracy: 0.9857\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0407 - binary_accuracy: 0.9857\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0618 - binary_accuracy: 0.9772\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0413 - binary_accuracy: 0.9857\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0347 - binary_accuracy: 0.9843\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0453 - binary_accuracy: 0.9815\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0527 - binary_accuracy: 0.9829\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0386 - binary_accuracy: 0.9829\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0305 - binary_accuracy: 0.9872\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0263 - binary_accuracy: 0.9900\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0244 - binary_accuracy: 0.9914\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0162 - binary_accuracy: 0.9943\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0229 - binary_accuracy: 0.9872\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0692 - binary_accuracy: 0.9729\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0331 - binary_accuracy: 0.9872\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0252 - binary_accuracy: 0.9886\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0163 - binary_accuracy: 0.9957\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0153 - binary_accuracy: 0.9971\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0110 - binary_accuracy: 0.9957\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0126 - binary_accuracy: 0.9971\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0292 - binary_accuracy: 0.9872\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0099 - binary_accuracy: 0.9957\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0079 - binary_accuracy: 0.9957\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0177 - binary_accuracy: 0.9914\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0098 - binary_accuracy: 0.9971\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0047 - binary_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0051 - binary_accuracy: 0.9986\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0027 - binary_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0023 - binary_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0033 - binary_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0293 - binary_accuracy: 0.9900\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0444 - binary_accuracy: 0.9886\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0406 - binary_accuracy: 0.9857\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0090 - binary_accuracy: 0.9971\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0052 - binary_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.0032 - binary_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0036 - binary_accuracy: 0.9986\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0014 - binary_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0013 - binary_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0011 - binary_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 6.6652e-04 - binary_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 9.0503e-04 - binary_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 7.0508e-04 - binary_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 5.4942e-04 - binary_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 5.1461e-04 - binary_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 2.4125e-04 - binary_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 3.4608e-04 - binary_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 7.6277e-04 - binary_accuracy: 1.0000\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 12ms/step - loss: 3.1547e-04 - binary_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 2.9818e-04 - binary_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0014 - binary_accuracy: 0.9986\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 3.9372e-04 - binary_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.9445e-04 - binary_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 2.7964e-04 - binary_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 1.4716e-04 - binary_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.0012 - binary_accuracy: 0.9986\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 3.0511e-04 - binary_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 1.3525e-04 - binary_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 2.1960e-04 - binary_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 6.1715e-04 - binary_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.6268e-04 - binary_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.0845e-04 - binary_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 2.0134e-04 - binary_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 1.1415e-04 - binary_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0011 - binary_accuracy: 0.9986\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 3.2731e-04 - binary_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 8.6684e-05 - binary_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 5.5702e-05 - binary_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.8316e-04 - binary_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 2.8380e-05 - binary_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.3551e-04 - binary_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.5692e-04 - binary_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 2.2058e-05 - binary_accuracy: 1.0000\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.91      0.89        69\n",
      "         1.0       0.94      0.92      0.93       107\n",
      "\n",
      "    accuracy                           0.91       176\n",
      "   macro avg       0.91      0.91      0.91       176\n",
      "weighted avg       0.92      0.91      0.92       176\n",
      "\n",
      "ROC AUC: 0.9635649464987133\n",
      "MCC: 0.823098992445235\n"
     ]
    }
   ],
   "source": [
    "#RNN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, matthews_corrcoef\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "# 数据预处理\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "input_shape = (1, X_train.shape[2])\n",
    "\n",
    "# 构建RNN模型\n",
    "model = Sequential()\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(32,return_sequences = True))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "# 编译模型\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# 在测试集上评估模型\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = y_pred.flatten()\n",
    "y_pred_binary = np.round(y_pred_proba)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "mcc = matthews_corrcoef(y_test, y_pred_binary)\n",
    "print(\"MCC:\", mcc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f0e2223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Sequence, Predictions, and Probabilities:\n",
      "[['HLPLP' 0 1.0]\n",
      " ['LQSGDALRVPSGTTYY' 1 1.0]\n",
      " ['LLPHHADADY' 1 1.0]\n",
      " ['LVNPHDHQN' 1 1.0]\n",
      " ['VNPHDHQN' 1 1.0]\n",
      " ['VHHANEN' 1 1.0]\n",
      " ['VHH' 1 1.0]\n",
      " ['AHK' 1 1.0]\n",
      " ['VLPIPQ' 0 0.02283494919538498]\n",
      " ['SVPQPK' 0 9.812758605676208e-08]\n",
      " ['VVKL' 1 7.661884460219426e-09]\n",
      " ['EVPKA' 0 4.805029707455333e-09]\n",
      " ['PLAQPA' 0 3.7031375743623585e-09]\n",
      " ['LLNPT' 0 1.0773045788781133e-09]\n",
      " ['FSL' 0 4.860063462786002e-10]\n",
      " ['IVF' 0 3.1987146265066713e-10]]\n"
     ]
    }
   ],
   "source": [
    "# 预测新的序列\n",
    "new_sequence_data = X_out  # 替换成您的新序列数据文件\n",
    "new_sequence = new_sequence_data.iloc[:, 0:].values\n",
    "new_sequence = new_sequence.reshape(new_sequence.shape[0], 1, new_sequence.shape[1])\n",
    "predictions = model.predict(new_sequence)\n",
    "predictions_proba = predictions.flatten()\n",
    "sorted_indices = np.argsort(predictions_proba)[::-1]\n",
    "\n",
    "sorted_indices_RNN = np.argsort(predictions_proba)[::-1]\n",
    "sorted_sequence_RNN = X_out_Sequence.iloc[sorted_indices_RNN]\n",
    "sorted_proba_RNN = predictions_proba[sorted_indices_RNN]\n",
    "output = np.column_stack((sorted_sequence_RNN, sorted_proba_RNN))\n",
    "\n",
    "print(\"Sorted Sequence, Predictions, and Probabilities:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27609a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880806bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
